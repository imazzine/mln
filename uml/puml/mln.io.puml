@startuml io

  node "Client container" as client_container {
    node "Client process (V8 support)" as client_process {
      node "Main thread" as client_main_thread {
        
      }
      interface "ws+http" as client_websocket
      client_main_thread --( client_websocket
    }
    interface "tcp/ip" as client_tcp_ip
    client_websocket --( client_tcp_ip
  }

  cloud "Network" as network
  client_tcp_ip -- network

  node "Router container" as router_container {
    interface "tcp/ip" as router_tcp_ip
    network --( router_tcp_ip

    node "Router process (V8 support)" as router_process {
      interface "ws+http" as router_websocket
      router_tcp_ip )- router_websocket

      interface "kafkajs" as router_kafka_js
      router_tcp_ip )-- router_kafka_js

      node "Main thread" as router_main_thread {
        component Router
        router_kafka_js )-- Router

        component Clients
        Router *- Clients

        component BrokersRegister
        Router *-- BrokersRegister

        component WorkersRegister
        Router *-- WorkersRegister
      }
      router_websocket )-- router_main_thread
    }
  }

  node "Service container" as service_container {
    interface "tcp/ip" as service_tcp_ip
    network --( service_tcp_ip

    node "Service process (V8 support)" as service_process {
      interface "ws+http" as service_websocket
      service_tcp_ip )- service_websocket

      interface "kafkajs" as service_kafka_js
      service_tcp_ip )-- service_kafka_js

      node "Main thread" as service_main_thread {
        component Worker
        service_kafka_js )-- Worker
      }
      service_websocket )-- service_main_thread
    }
  }

  queue "Kafka protocol" as kafka_protocol
  router_tcp_ip )-- kafka_protocol
  service_tcp_ip )-- kafka_protocol

@enduml

@startuml topics

  object Headers {
    +tenant: string
    +token: string
  }

  object Brokers {
    +key: string
    +partition [f(key)]: string
    +timestamp: string
    +headers: Headers
    +value: BrokerRegistered | BrokerRemoved
  }
  Headers *-- Brokers

  object BrokerRegistered {
    +uuid: string
  }
  Brokers --o BrokerRegistered

  object BrokerRemoved {
    +uuid: string
  }
  Brokers --o BrokerRemoved

  object "Heartbeats[uuid]" as Heartbeats {
    +key: string
    +partition [f(key)]: string
    +timestamp: string
    +headers: Headers
    +value: Heartbeat
  }
  Headers *-- Heartbeats

  object Heartbeat {
    +uuid: string
  }
  Heartbeats --o Heartbeat

  object "Workers" as Workers {
    +key: string
    +partition [f(key)]: string
    +timestamp: string
    +headers: Headers
    +value: WorkerRegistered | WorkerRemoved
  }
  Headers *-- Workers

  object WorkerRegistered {
    +uuid: string
    +services: string[]
  }
  Workers --o WorkerRegistered

  object WorkerRemoved {
    +uuid: string
    +services: string[]
  }
  Workers --o WorkerRemoved

  object Requests {
    +key: string
    +partition [f(key)]: string
    +timestamp: string
    +headers: Headers
    +value: Request
  }
  Headers *-- Requests

  object Request {
    +type: "open" | "close"
    +broker: string
    +service: string
    +inbound: string
    +outbound: string
  }
  Requests *-- Request

  object Responses {
    +key: string
    +partition [f(key)]: string
    +timestamp: string
    +headers: Headers
    +value: Response
  }
  Headers *-- Responses

  object Response {
    +type: "open" | "close"
    +broker: string
    +service: string
    +inbound: string
    +outbound: string
    +approved: boolean
  }
  Responses *-- Response

@enduml

@startuml router_up

  participant "Node" as Broker
  queue "mln.io" as Io

  group registration
    Broker --> Io: send register message
    Broker <-- Io: ack
  end

  group orchestration
    group brokers
      Broker --> Io: subscribe from the beginning
      Broker <-- Io: messages
      loop for each message
        alt registered
          Broker --> Broker: add registered broker to the list
          Broker --> Io: subscribe for the registered heartbeat queue
          Broker <-- Io: ack
        else removed
          Broker --> Broker: remove broker from the list
          Broker --> Io: unsubscribe from the registered heartbeat queue
          Broker <-- Io: ack
        end
      end
    end
    group workers
      Broker --> Io: subscribe from the beginning
      Broker <-- Io: messages
      loop for each message
        alt registered
          Broker --> Broker: add registered worker to the list
          Broker --> Io: subscribe for the registered heartbeat queue
          Broker <-- Io: ack
        else removed
          Broker --> Broker: remove worker from the list
          Broker --> Io: unsubscribe from the registered heartbeat queue
          Broker <-- Io: ack
        end
      end
    end
  end

  group heartbeating
    Broker --> Io: heartbeat message
    Broker <-- Io: ack
  end

@enduml

@startuml worker_up

  participant Worker
  queue Heartbeat
  queue Requests
  queue Responses
  queue Workers

  group registration
    Worker --> Heartbeat **: create heartbeat queue
    Worker <-- Heartbeat: ack
    Worker --> Requests **
    Worker <-- Requests: ack
    Worker --> Responses **
    Worker <-- Responses: ack
    Worker --> Workers: send registered message
    Worker <-- Workers: ack
  end

  group heartbeating
    Worker --> Heartbeat: heartbeat message
    Worker <-- Heartbeat: ack
  end

@enduml

@startuml types

  class mln.io.Register extends mln.Node {
    -interval: number
    -register: Map<string, timeoutID>
    -sendHeartbeat(): Promise<boolean>
    -processHeartbeat(uid: string, timestamp: string): void
  }

  class mln.io.BrokersRegister extends mln.io.Register {
  }

  class mln.io.WorkersRegister extends mln.io.Register {
    -services: Map<string, string[]>
  }

  class mln.io.Worker extends mln.Node {
    -services: string[]
    +constructor(services: string[]): Worker
  }

@enduml

@startuml sequence

  participant "Node 1" as Node1
  participant "Node 2" as Node2
  participant "Node N" as NodeN

  database "Kafka API" as kafkaIO
  queue "uid1" as uid1IO
  queue "uid2" as uid2IO
  queue "uidN" as uidNIO

  group start
    -> Node1: running node
    Node1 -> uid1IO **: create topic
    Node1 <- uid1IO: ack
    Node1 -> Node1: ready
  end

  group ping
    Node1 -> kafkaIO: list_groups
    Node1 <- kafkaIO: list_groups
    Node1 -> Node1: sync: workers
  end

  group start
    -> Node2: running node
    Node2 -> uid2IO **: create topic
    Node2 <- uid2IO: ack
    Node2 -> Node2: ready
  end

  group ping
    Node2 -> kafkaIO: list_groups
    Node2 <- kafkaIO: list_groups
    Node2 -> Node2: sync: workers
  end

  group sync
    Node2 -> uid1IO: sync: [uid2, uid1], resources
    Node1 <- uid1IO: sync: [uid2, uid1], resources
    Node1 -> Node1: updated: uid2, resources

    Node1 -> uid2IO: sync: [uid2, uid1], resources
    Node2 <- uid2IO: sync: [uid2, uid1], resources
    Node2 -> Node2: updated: uid1, resources
  end

  group start
    -> NodeN: running node
    NodeN -> uidNIO **
    NodeN <- uidNIO: ack
    NodeN -> NodeN: ready
  end

  group ping
    NodeN -> kafkaIO: list_groups
    NodeN <- kafkaIO: list_groups
    NodeN -> NodeN: sync: workers
  end

  group sync
    NodeN -> uid1IO: sync: [uidN, uid1], resources
    Node1 <- uid1IO: sync: [uidN, uid1], resources
    Node1 -> Node1: updated: uidN, resources

    Node1 -> uidNIO: sync: [uidN, uid1], resources
    NodeN <- uidNIO: sync: [uidN, uid1], resources
    NodeN -> NodeN: updated: uid1, resources
  end

  group sync
    NodeN -> uid2IO: sync: [uidN, uid2], resources
    Node2 <- uid2IO: sync: [uidN, uid2], resources
    Node2 -> Node2: updated: uidN, resources

    Node2 -> uidNIO: sync: [uidN, uid2], resources
    NodeN <- uidNIO: sync: [uidN, uid2], resources
    NodeN -> NodeN: updated: uid2, resources
  end

  group ping
    Node1 -> kafkaIO: list_groups
    Node1 <- kafkaIO: list_groups
    Node1 -> Node1: sync: workers
  end

  group ping
    Node2 -> kafkaIO: list_groups
    Node2 <- kafkaIO: list_groups
    Node2 -> Node2: sync: workers
  end

  group ping
    NodeN -> kafkaIO: list_groups
    NodeN <- kafkaIO: list_groups
    NodeN -> NodeN: sync: workers
  end

  group connect
    group rate
      Node1 -> Node1: rate: uid1, resource, number
    end

    group rate
      Node1 -> uid2IO: rate: [uid1, uid2], resource
      Node2 <- uid2IO: rate: [uid1, uid2], resource
      Node2 -> uid2IO: rate: [uid1, uid2], resource, number
      Node1 <- uid2IO: rate: [uid1, uid2], resource, number
      Node1 -> Node1: rate: uid2, resource, number
    end

    group rate
      Node1 -> uidNIO: rate: [uid1, uidN], resource
      NodeN <- uidNIO: rate: [uid1, uidN], resource
      NodeN -> uidNIO: rate: [uid1, uidN], resource, number
      Node1 <- uidNIO: rate: [uid1, uidN], resource, number
      Node1 -> Node1: rate: uidN, resource, number
    end

    Node1 -> Node1: connecting: uidN
  end

  group connecting
    Node1 -> Node1: stream: [uid1, uidN]
    Node1 -> uidNIO: stream: [uid1, uidN]
    NodeN <- uidNIO: stream: [uid1, uidN]
    NodeN -> NodeN: stream: [uid1, uidN]
    NodeN -> uid1IO: stream: [uid1, uidN]
    NodeN -> NodeN: connected: [uid1, [in, out]]
    Node1 <- uid1IO: stream: [uid1, uidN]
    Node1 -> Node1: connected: [uidN, [in, out]]
  end

  group connected
    Node1 -> uidNIO: data
    NodeN <- uidNIO: data
    NodeN -> uid1IO: data
    Node1 <- uid1IO: data
  end

  group disconnect
    NodeN -> NodeN: disconnected: uid1
    NodeN -> uid1IO: unstream: [uidN, uid1]
    Node1 <- uid1IO: unstream: [uidN, uid1]
    Node1 -> Node1: disconnected: uidN
    Node1 -> Node1: unstream: [uidN, uid1]
    Node1 -> uidNIO: unstream: [uidN, uid1]
    NodeN <- uidNIO: unstream: [uidN, uid1]
    NodeN -> NodeN: unstream: [uidN, uid1]
  end

@enduml

@startuml Worker

  object process.env {
    +MLN_KAFKA_HOSTS: localhost:9092,127.0.0.1:9092
    ....
    +MLN_KAFKA_CONNECTION_TIMEOUT: default 10000 ms
    +MLN_KAFKA_CONNECTION_IDLE: default 300000 ms
    ....
    +MLN_KAFKA_REQUEST_TIMEOUT: default 30000 ms
    +MLN_KAFKA_REQUEST_MAX_ASYNC: default 10
    ....
    +MLN_KAFKA_PRODUCER_HIGH_WATER_MARK: default 100
    +MLN_KAFKA_PRODUCER_ACK_TIMEOUT: default 100 ms
    ....
    +MLN_KAFKA_CONSUMER_HIGH_WATER_MARK: default 100
    +MLN_KAFKA_CONSUMER_FETCH_MAX_WAIT: default 100 ms
    +MLN_KAFKA_CONSUMER_FETCH_MIN_BYTES: default 1
    +MLN_KAFKA_CONSUMER_FETCH_MAX_BYTES: default 1024 * 1024
  }

  class Worker {
    -_client: kafka.KafkaClient
    -_producer: kafka.ProducerStream
    -_consumer: kafka.ConsumerStream
    -_resources: Map<resource, workers[]>
    -_workers: Map<worker, resources[]>
    -_connections: Map<worker, [out: Readable, in: Writable]>
    ....
    #getRate(resource: string): number
    ....
    +ping(): void
    +sync(): void
    +rate(resource: string): void
    ....
    +register(resource: string, service: Service): void
    +unregister(resource: string): void
    ....
    +connect(worker: string): void
    +disconnect(worker: string): void
  }
  process.env --> Worker: use to connect to the kafka

  class Ready {
    +type: "ready"
  }
  Ready <- Worker: dispatch

  class Pong {
    +type: "pong"
    +scope: workers
  }
  Pong <- Worker: .ping() -> .dispatch(Pong)

  class Sync {
    +type: "updated"
    +scope: [worker, resources]
  }
  Worker --> Sync: dispatch

  class Removed {
    +type: "removed"
    +scope: [worker, resources]
  }
  Worker --> Removed: dispatch

  class Rate {
    +type: "rate"
    +scope: [ worker, resource, number ]
  }
  Worker --> Rate: dispatch

  class Connecting {
    +type: "connecting"
    +scope: worker
  }
  Worker --> Connecting: dispatch

  class Stream {
    +type: "stream"
    +scope: [worker, worker]
  }
  Worker --> Stream: dispatch

  class Connected {
    +type: "connected"
    +scope: [worker, [out, in]]
  }
  Worker --> Connected: dispatch

  class Disonnected {
    +type: "disconnected"
    +scope: worker
  }
  Worker -> Disonnected: dispatch

  class Unstream {
    +type: "unstream"
    +scope: [worker, worker]
  }
  Worker -> Unstream: dispatch

@enduml